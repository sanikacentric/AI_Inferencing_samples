{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4NMJ_JFpl6j"
      },
      "outputs": [],
      "source": [
        "# groq_inference_app.py (Conceptual, not executable without Groq access)\n",
        "\n",
        "import requests\n",
        "\n",
        "def run_groq_inference(prompt):\n",
        "    url = \"https://api.groq.com/inference\"\n",
        "    headers = {\n",
        "        \"Authorization\": \"Bearer YOUR_GROQ_API_KEY\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"groq-llm\",\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 50\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    return response.json()[\"generated_text\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = \"AI will revolutionize\"\n",
        "    output = run_groq_inference(prompt)\n",
        "    print(\"Groq Inference Output:\\n\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No GPU needed — Hugging Face handles it. Ideal for small projects, demos, and testing."
      ],
      "metadata": {
        "id": "3ryLP-B9pyTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
        "headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN\"}\n",
        "\n",
        "def query(prompt):\n",
        "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
        "    return response.json()[0]['generated_text']\n",
        "\n",
        "print(query(\"AI is revolutionizing the world because\"))\n"
      ],
      "metadata": {
        "id": "ZTFYnP6Tpwn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Example: ONNX Runtime (Fast local inference)\n",
        "Convert a model to ONNX and run:If you want ONNX and local control, go with Phi‑3.5‑mini‑instruct or Phi‑3‑mini—they're powerful, ONNX-ready, and openly accessible"
      ],
      "metadata": {
        "id": "kRsbwuI_p5Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnxruntime transformers"
      ],
      "metadata": {
        "id": "FdxzVqYZp6Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from onnxruntime import InferenceSession\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "session = InferenceSession(\"phi3.5-mini-instruct-128k.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"onnx-community/Phi-3.5-mini-instruct-onnx-web\")\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(\"Explain ONNX.\", return_tensors=\"np\")\n",
        "outputs = session.run(None, {session.get_inputs()[0].name: inputs['input_ids']})\n",
        "\n",
        "print(\"ONNX output:\", outputs[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "kM41t6dUp8Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vLLM"
      ],
      "metadata": {
        "id": "R1xNHArmrWHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch vLLM with a model you own\n",
        "python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model mistralai/Mistral-7B-Instruct-v0.2\n"
      ],
      "metadata": {
        "id": "_OlXlxZqrXaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_base = \"http://localhost:8000/v1\"\n",
        "openai.api_key = \"dummy\"\n",
        "\n",
        "response = client.ChatCompletion.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain ONNX in simple terms.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message[\"content\"])\n"
      ],
      "metadata": {
        "id": "3Fon7bYirYOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}